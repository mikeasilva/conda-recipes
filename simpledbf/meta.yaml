{% set name = "simpledbf" %}
{% set version = "0.2.6" %}
{% set file_ext = "tar.gz" %}
{% set hash_type = "sha256" %}
{% set hash_value = "345d4f1e58549c4a6ad8079d6ed65ad8550ccdc1acd8444a946cdb04d0212af4" %}

package:
  name: '{{ name|lower }}'
  version: '{{ version }}'

source:
  fn: '{{ name }}-{{ version }}.{{ file_ext }}'
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.{{ file_ext }}
  '{{ hash_type }}': '{{ hash_value }}'

build:
  number: 0
  script: python setup.py install --single-version-externally-managed --record=record.txt

requirements:
  host:
    - python
    - setuptools
  run:
    - python

test:
  imports:
    - simpledbf

about:
  home: https://github.com/rnelsonchem/simpledbf
  license: BSD License
  license_family: BSD
  license_file: ''
  summary: Convert DBF files to CSV, DataFrames, HDF5 tables, and SQL tables. Python3 compatible.
  description: "simpledbf\n#########\n\n*simpledbf* is a Python library for converting basic DBF files (see\n`Limitations`_) to CSV files, Pandas DataFrames, SQL tables, or HDF5 tables.\nThis package is\
    \ fully compatible with Python >=3.4, with almost complete\n`Python 2.7 support`_ as well. The conversion to CSV and SQL (see\n``to_textsql`` below) is entirely written in Python, so no additional\n\
    dependencies are necessary. For other export formats, see `Optional\nRequirements`_.  This code was designed to be very simple, fast and memory\nefficient for convenient interactive or batch file processing;\
    \ therefore, it\nlacks many features, such as the ability to write DBF files, that other\npackages might provide. \n\nBug fixes, questions, and update requests are encouraged and can be filed at\nthe\
    \ `GitHub repo`_. \n\nThis code is derived from an  `ActiveState DBF example`_ that works with\nPython2 and is distributed under a PSF license.\n\n\n.. _Optional Requirements:\n\nOptional Requirements\n\
    ---------------------\n\n* Pandas >= 0.15.2 (Required for DataFrame)\n\n* PyTables >= 3.1 (with Pandas required for HDF tables)\n\n* SQLalchemy >= 0.9 (with Pandas required for DataFrame-SQL tables)\n\
    \nInstallation\n------------\n\nThe most recent release of *simpledbf* can be installed using ``pip`` or\n``conda``, if you happen to be using the `Anaconda Python distribution`_.\n\nUsing ``conda``::\n\
    \n    $ conda install -c https://conda.binstar.org/rnelsonchem simpledbf\n\nUsing ``pip``::\n\n    $ pip install simpledbf\n\nThe development version can be installed from GitHub::\n\n    $ pip install\
    \ git+https://github.com/rnelsonchem/simpledbf.git\n\nAs an alternative, this package only contains a single file, so in principle,\nyou could download the ``simpledbf.py`` file from Github and put\
    \ it in any\nfolder of your choosing.\n\n\n.. _Limitations:\n\nDBF File Limitations\n--------------------\n\nThis package currently supports a subset of `dBase III through 5`_ DBF files.\nIn particular,\
    \ support is missing for linked memo (i.e. DBT) files. This is\nmostly due to limitations in the types of files available to the author.  Feel\nfree to request an update if you can supply a DBF file\
    \ with an associated memo\nfile. `DBF version 7`_, the most recent DBF file spec, is not currently\nsupported by this package.\n\n\n.. _Python 2.7 support:\n\nPython 2 Support \n----------------\n\n\
    Except for HDF file export, this code should work fine with Python >=2.7.\nHowever, HDF files created in Python3 are compatible with all Python2 HDF\npackages, so in principle, you could make any HDF\
    \ files in a temporary Python3\nenvironment. If you are using the `Anaconda Python distribution`_\n(recommended), then you can make a small Python3 working environment as\nfollows:\n\n.. code::\n\n\
    \    $ conda create -n dbf python=3 pandas pytables sqlalchemy\n    # Lots of output...\n    \n    $ source activate dbf\n\n    dbf>$ conda install -c https://conda.binstar.org/rnelsonchem simpledbf\n\
    \n    dbf>$ python my_py3_hdf_creation_script.py\n    # This is using Python3\n\n    dbf>$ source deactivate\n\n    $ python my_py2_stuff_with_hdf.py\n    # This is using Python2 again\n\nHDF file export\
    \ is currently broken in Python2 due to a `limitation in Pandas\nHDF export with unicode`_. This issue may be fixed future versions of\nPandas/PyTables.\n\n\nExample Usage\n#############\n\n.. _Loading:\n\
    \nLoad a DBF file\n---------------\n\nThis module currently only defines a single class, ``Dbf5``, which is\ninstantiated with a DBF file name, which can contain path info as well. An\noptional 'codec'\
    \ keyword argument that controls the codec used for\nreading/writing files. The default is 'utf-8'. See the documentation for\nPython's `codec standard library module`_ for more codec options.\n\n..\
    \ code::\n\n    In : from simpledbf import Dbf5\n\n    In : dbf = Dbf5('fake_file_name.dbf', codec='utf-8')\n\nThe ``Dbf5`` object initially only reads the header information from the file,\nso you\
    \ can inspect some of the properties. For example, ``numrec`` is the\nnumber of records in the DBF file, and ``fields`` is a list of tuples with\ninformation about the data columns. See the DBF file\
    \ spec for info on the\ncolumn type characters. The \"DeletionFlag\" column is always present as a check\nfor deleted records; however, it is never exported during conversion.\n\n.. code::\n\n    In\
    \ : dbf.numrec\n    Out: 10000\n\n    In : dbf.fields\n    Out: [('DeletionFlag', 'C', 1), ('col_1', 'C', 15), ('col_2', 'N', 2)]\n\nThe docstring for this object contains a complete listing of attributes\
    \ and\ntheir descriptions.\n\nThe ``mem`` method gives an approximate memory requirement for processing this\nDBF file. (~2x the total file size, which could be wildly inaccurate.) In\naddition, all\
    \ of the output methods in this object take a ``chunksize``\nkeyword argument, which lets you split up the processing of large files into\nsmaller chunks to limit the total memory usage of the conversion\
    \ process. When\nthis keyword argument is passed into ``mem``, the approximate memory footprint\nof the chunk will also be given, which can be useful when trying to determine\nthe maximum chunksize\
    \ your memory will allow.\n\n.. code::\n\n    In : dbf.mem()\n    This total process would require more than 350.2 MB of RAM. \n\n    In : dbf.mem(chunksize=1000)\n    Each chunk will require 4.793\
    \ MB of RAM.\n    This total process would require more than 350.2 MB of RAM.\n\n\nExport the Data\n---------------\n\nThe ``Ddb5`` object behaves like Python's file object in that it will be\n\"exhausted\"\
    \ after export. To re-export the DBF data to a different format,\nfirst create a new ``Dbf5`` instance using the same file name. This procedure\nis followed in the documentation below.\n\n    \nNote\
    \ on Empty/Bad Data\n++++++++++++++++++++++\n\nThis package attempts to convert most blank strings and poorly formatted\nvalues to an empty value of your choosing. This is controlled by the ``na``\n\
    keyword argument to all export functions. The default for CSV is an empty\nstring (''), and for all other exports, it is 'nan' which converts empty/bad\nvalues to ``float('nan')``. *NOTE* The exception\
    \ here is that float/int\ncolumns always use ``float('nan')`` for all missing values for\nDBF->SQL->DataFrame conversion purposes. Pandas has very powerful functions\nfor `working with missing data`_,\
    \ including converting NaN to other values\n(e.g.  empty strings). \n\n        \nTo CSV\n++++++\n\nUse the ``to_csv`` method to export the data to a CSV file. This method\nrequires the name of a CSV\
    \ file as an input. The default behavior is to append\nnew data to an existing file, so be careful if the file already exists. The\n``chunksize`` keyword argument controls the frequency that  the file\
    \ buffer\nwill be flushed, which may not be necessary. The ``na`` keyword changes the\nvalue used for missing/bad entries (default is ''). The keyword ``header`` is\na boolean that controls writing\
    \ of the column names as the first row of the\nCSV file. The encoding of the resulting CSV file is determined by the codec\nthat is set when opening the DBF file, see `Loading`_. \n\n.. code::\n\n \
    \   In : dbf = Dbf5('fake_file_name.dbf')\n\n    In : dbf.to_csv('junk.csv')\n\nIf you are unhappy with the default CSV output of this module, Pandas also has\nvery `powerful CSV export capabilities`_\
    \ for DataFrames.\n\n\nTo SQL (CSV-based)\n++++++++++++++++++\n\nMost SQL databases can create tables directly from local CSV files. The\npure-Python ``to_textsql`` method creates two files: 1) a header-less\
    \ CSV file\ncontaining the DBF contents, and 2) a SQL file containing the appropriate\ntable creation and CSV import code. It is up to you to run the SQL file as a\nseparate step. This function takes\
    \ two mandatory arguments, which are simply\nthe names of the SQL and CSV files, respectively. In addition, there are a\nnumber of optional keyword arguments as well. ``sqltype`` controls the output\n\
    dialect. The default is 'sqlite', but 'postgres' is also accepted.  ``table``\nsets the name of the SQL table that will be created. By default, this will be\nthe name of the DBF file without the file\
    \ extension. You should escape quote\ncharacters (\") in the CSV file. This is controlled with the ``escapeqoute``\nkeyword, which defaults to ``'\"'``. (This changes '\"' in text strings to '\"\"',\n\
    which the SQL server should ignore.) The ``chunksize``, ``na``, and ``header``\nkeywords are used to control the CSV file. See above.\n\nHere's an example for SQLite:\n\n.. code::\n\n    In : dbf =\
    \ Dbf5('fake_file_name.dbf')\n\n    In : dbf.to_textsql('junk.sql', 'junk.csv')\n\n    # Exit Python\n    $ sqlite3 junk.db < junk.sql\n\nHere's an example for Postgresql:\n\n.. code::\n\n    In : dbf\
    \ = Dbf5('fake_file_name.dbf')\n\n    In : dbf.to_textsql('junk.sql', 'junk.csv', sqltype='postgres')\n\n    # Exit Python\n    $ psql -U username -f junk.sql db_name\n\nTo DataFrame \n++++++++++++\n\
    \nThe ``to_dataframe`` method returns the DBF records as a Pandas DataFrame.  If\nthe size of the DBF file exceeds available memory, then passing the\n``chunksize`` keyword argument will return a generator\
    \ function. This\ngenerator yields DataFrames of len(<=chunksize) until all of the records have\nbeen processed. The ``na`` keyword changes the value used for missing/bad\nentries (default is 'nan'\
    \ which inserts ``float('nan')``).\n\n.. code::\n\n    In : dbf = Dbf5('fake_file_name.dbf')\n\n    In : df = dbf.to_dataframe()\n    # df is a DataFrame with all records\n\n    In : dbf = Dbf5('fake_file_name.dbf')\n\
    \n    In : for df in dbf.to_dataframe(chunksize=10000)\n    ....     do_cool_stuff(df)\n    # Here a generator is returned\n\n.. _chunksize issue:\n\nIssue with DataFrame Chunksize\n++++++++++++++++++++++++++++++\n\
    \nWhen a DataFrame is constructed, it attempts to determine the dtype of each\ncolumn. If you chunk the DataFrame output, it turns out that the dtype for a\ncolumn can change. For example, if one chunk\
    \ has a column with all strings,\nthe dtype will be ``np.object``; however, if in the next chunk that same\ncolumn is full of ``float('nan')``, the resulting dtype will be set as\n``float``. This has\
    \ some consequences for writing to SQL and HDF tables as\nwell. In principle, this behavior could be changed, but it is currently\nnon-trivial to set the dtypes for DataFrame columns on construction.\
    \ Please\nfile a PR through GitHub if this is a big problem.\n\n\nTo an SQL Table using Pandas\n++++++++++++++++++++++++++++\n\nThe ``to_pandassql`` method will transfer the DBF entries to an SQL database\n\
    table of your choice using a combination of Pandas DataFrames and SQLalchemy.\nA valid `SQLalchemy engine string`_ argument is required to connect with the\ndatabase. Database support will be limited\
    \ to those supported by SQLalchemy.\n(This has been tested with SQLite and Postgresql.) Note, if you are\ntransferring a large amount of data, this method will be very slow. If you\nhave direct access\
    \ to the SQL server, you might want to use the text-based SQL\nexport instead.\n\n.. code::\n\n    In : dbf = Dbf5('fake_file_name.dbf')\n\n    In : dbf.to_pandassql('sqlite:///foo.db')\n\nThis method\
    \ accepts three optional arguments. ``table`` is the name of the\ntable you'd like to use. If this is not passed, your new table will have the\nsame name as the DBF file without file extension. Again,\
    \ the default here is\nto append to an existing table. If you want to start fresh, delete the\nexisting table before using this function. The ``chunksize`` keyword processes\nthe DBF file in chunks\
    \ of records no larger than this size. The ``na`` keyword\nchanges the value used for missing/bad entries (default is 'nan' which inserts\n``float('nan')``).\n\n.. code::\n\n    In : dbf = Dbf5('fake_file_name.dbf')\n\
    \n    In : dbf.to_pandassql('sqlite:///foo.db', table=\"fake_tbl\",\n    ....                    chunksize=100000)\n    \n\nTo an HDF5 Table\n++++++++++++++++\n\nThe ``to_pandashdf`` method transfers\
    \ the DBF entries to an HDF5 table of your\nchoice. This method uses a combination of Pandas DataFrames and PyTables, so\nboth of these packages must be installed. This method requires a file name\n\
    string for the HDF file, which will be created if it does not exist.  Again,\nthe default behavior is to append to an existing file of that name, so be\ncareful here.  The HDF file will be created using\
    \ the highest level of\ncompression (9) with the 'blosc' compression lib. This saves an enormous\namount of disk space, with little degradation of performance; however, this\ncompression library is\
    \ non-standard, which can cause problems with other HDF\nlibraries. Compression options are controlled use the ``complib`` and\n``complevel`` keyword arguments, which are identical to the ones described\
    \ in\nthe `Pandas HDF compression docs`_.\n\n.. code::\n\n    In : dbf = Dbf5('fake_file_name.dbf')\n\n    In : dbf.to_pandashdf('fake.h5')\n\nThis method uses the same optional arguments, and corresponding\
    \ defaults, as\n``to_pandassql`` (see above). A example with ``chunksize`` is shown below. In\naddition, a ``data_columns`` keyword argument is also available, which sets\nthe columns that will be used\
    \ as data columns in the HDF table. Data columns\ncan be used for advanced searching and selection; however, there is some\ndegredation of preformance for large numbers of data columns. See the `Pandas\n\
    data columns docs`_ for a more detailed explanation.\n\n.. code::\n\n    In : dbf = Dbf5('fake_file_name.dbf')\n\n    In : dbf.to_pandashdf('fake.h5', table=\"fake_tbl\", chunksize=100000)\n\nSee the\
    \ `chunksize issue`_ for DataFrame export for information on a potential\nproblem you may encounter with chunksize.\n\n\nBatch Export\n++++++++++++\n\nBatch file export is trivial using *simpledbf*.\
    \ For example, the following\ncode processes all DBF files in the current directory into separate tables in\na single HDF file.\n\n.. code:: \n\n    In : import os\n\n    In : from simpledbf import\
    \ Dbf5\n\n    In : files = os.listdir('.')\n\n    In : for f in files:\n    ....     if f[-3:].lower() == 'dbf':\n    ....         dbf = Dbf5(f)\n    ....         dbf.to_pandashdf('all_data.h5')\n\n\
    \   \n.. External Hyperlinks\n\n.. _ActiveState DBF example: http://code.activestate.com/recipes/\n        362715-dbf-reader-and-writer/\n.. _GitHub repo: https://github.com/rnelsonchem/simpledbf\n\
    .. _dBase III through 5: http://ulisse.elettra.trieste.it/services/doc/\n        dbase/DBFstruct.htm\n.. _DBF version 7: http://www.dbase.com/KnowledgeBase/int/db7_file_fmt.htm\n.. _Anaconda Python\
    \ distribution: http://continuum.io/downloads\n.. _limitation in Pandas HDF export with unicode: http://pandas.pydata.org/\n        pandas-docs/stable/io.html#datatypes\n.. _codec standard library module:\
    \ https://docs.python.org/3.4/library/\n        codecs.html \n.. _working with missing data: http://pandas.pydata.org/pandas-docs/stable/\n        missing_data.html\n.. _powerful CSV export capabilities:\
    \ http://pandas.pydata.org/pandas-docs/\n        stable/io.html#writing-to-csv-format\n.. _SQLalchemy engine string: http://docs.sqlalchemy.org/en/rel_0_9/core/\n        engines.html\n.. _Pandas HDF\
    \ compression docs: http://pandas.pydata.org/pandas-docs/stable/\n        io.html#compression\n.. _Pandas data columns docs: http://pandas.pydata.org/pandas-docs/stable/\n        io.html#query-via-data-columns"
  doc_url: ''
  dev_url: ''

extra:
  recipe-maintainers: ''
